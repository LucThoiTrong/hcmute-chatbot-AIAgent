import sys
import os
import glob
from pathlib import Path

sys.path.append(str(Path(__file__).resolve().parent.parent))

from langchain_community.document_loaders import Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client.models import VectorParams, Distance

# Import t·ª´ h·∫° t·∫ßng
from core.config import settings
from infrastructure.db_connector import get_qdrant_client, get_vector_store
from infrastructure.ai_connector import get_embeddings

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n d·ªØ li·ªáu
DATA_FOLDER = r"E:\NCKH_TLCN_KLTN\Data"


def import_documents():
    print(f"üìÇ ƒêang qu√©t d·ªØ li·ªáu t·ª´ folder: {DATA_FOLDER}")

    # --- B∆Ø·ªöC 1: LOAD V√Ä SPLIT DATA ---
    docx_files = glob.glob(os.path.join(DATA_FOLDER, "*.docx"))

    if not docx_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file .docx n√†o!")
        return

    print(f"üîé T√¨m th·∫•y {len(docx_files)} files.")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=200,
        chunk_overlap=100,
        separators=["\n\n", "\n", ".", " ", ""]
    )

    all_chunks = []

    for file_path in docx_files:
        file_name = os.path.basename(file_path)
        print(f"   -> Processing: {file_name}...")

        try:
            loader = Docx2txtLoader(file_path)
            documents = loader.load()

            chunks = text_splitter.split_documents(documents)

            # G√°n metadata ƒë·ªÉ truy xu·∫•t ngu·ªìn g·ªëc sau n√†y
            for chunk in chunks:
                chunk.metadata["source"] = file_name
                all_chunks.append(chunk)

        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc file {file_name}: {e}")

    if not all_chunks:
        print("‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu n√†o ƒë·ªÉ import.")
        return

    print(f"üì¶ T·ªïng c·ªông ƒë√£ t·∫°o ra {len(all_chunks)} chunks d·ªØ li·ªáu.")

    # --- B∆Ø·ªöC 2: CHU·∫®N B·ªä COLLECTION (D√πng Raw Client) ---
    # M·ª•c ƒë√≠ch: ƒê·∫£m b·∫£o Collection t·ªìn t·∫°i v·ªõi ƒë√∫ng c·∫•u h√¨nh tr∆∞·ªõc khi LangChain ƒë·∫©y data v√†o
    client = get_qdrant_client()
    collection_name = settings.QDRANT_COLLECTION_NAME

    # L·∫•y k√≠ch th∆∞·ªõc vector m·∫´u t·ª´ model Azure
    embeddings_model = get_embeddings()
    sample_embedding = embeddings_model.embed_query("test")
    vector_size = len(sample_embedding)

    if not client.collection_exists(collection_name):
        print(f"üÜï T·∫°o m·ªõi Collection '{collection_name}' (size={vector_size}, distance=Cosine)...")
        client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),
        )
    else:
        print(f"‚ÑπÔ∏è Collection '{collection_name}' ƒë√£ t·ªìn t·∫°i. S·∫µn s√†ng ghi th√™m d·ªØ li·ªáu.")

    # --- B∆Ø·ªöC 3: EMBED V√Ä STORE (D√πng LangChain Vector Store) ---
    print("üöÄ B·∫Øt ƒë·∫ßu Embedding v√† Upload l√™n Qdrant th√¥ng qua LangChain...")

    try:
        vector_store = get_vector_store()

        # H√†m n√†y l√†m t·∫•t c·∫£: Embed -> T·∫°o ID -> Batching -> Upsert
        # N√≥ tr·∫£ v·ªÅ list c√°c ID ƒë√£ l∆∞u th√†nh c√¥ng
        ids = vector_store.add_documents(documents=all_chunks)

        print(f"‚úÖ Ho√†n t·∫•t! ƒê√£ l∆∞u th√†nh c√¥ng {len(ids)} vectors v√†o Qdrant.")

    except Exception as e:
        print(f"‚ùå L·ªói trong qu√° tr√¨nh l∆∞u vector: {e}")


if __name__ == "__main__":
    import_documents()
