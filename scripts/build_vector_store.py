import sys
import os
import glob
import uuid
from pathlib import Path

sys.path.append(str(Path(__file__).resolve().parent.parent))

from langchain_community.document_loaders import Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from qdrant_client.models import PointStruct, VectorParams, Distance

# Import c√°c module h·∫° t·∫ßng ƒë√£ x√¢y d·ª±ng
from core.config import settings
from infrastructure.db_connector import get_qdrant_client
from infrastructure.ai_connector import get_embeddings

# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n d·ªØ li·ªáu
DATA_FOLDER = r"E:\NCKH_TLCN_KLTN\Data"


def import_documents():
    print(f"üìÇ ƒêang qu√©t d·ªØ li·ªáu t·ª´ folder: {DATA_FOLDER}")

    # 1. T√¨m t·∫•t c·∫£ file .docx trong th∆∞ m·ª•c
    docx_files = glob.glob(os.path.join(DATA_FOLDER, "*.docx"))

    if not docx_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y file .docx n√†o!")
        return

    print(f"üîé T√¨m th·∫•y {len(docx_files)} files.")

    # 2. Load v√† Split text
    # Splitter gi√∫p chia vƒÉn b·∫£n d√†i th√†nh ƒëo·∫°n nh·ªè (kho·∫£ng 1000 k√Ω t·ª±), ch·ªìng l·∫•n 200 k√Ω t·ª± ƒë·ªÉ gi·ªØ ng·ªØ c·∫£nh
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\n\n", "\n", ".", " ", ""]
    )

    all_chunks = []

    for file_path in docx_files:
        file_name = os.path.basename(file_path)
        print(f"   -> Processing: {file_name}...")

        try:
            # Python equivalent c·ªßa DocxLoader
            loader = Docx2txtLoader(file_path)
            documents = loader.load()

            # C·∫Øt nh·ªè vƒÉn b·∫£n
            chunks = text_splitter.split_documents(documents)

            # Th√™m metadata (t√™n file) ƒë·ªÉ sau n√†y bi·∫øt ngu·ªìn tr√≠ch d·∫´n
            for chunk in chunks:
                chunk.metadata["source"] = file_name
                all_chunks.append(chunk)

        except Exception as e:
            print(f"‚ö†Ô∏è L·ªói khi ƒë·ªçc file {file_name}: {e}")

    print(f"üì¶ T·ªïng c·ªông ƒë√£ t·∫°o ra {len(all_chunks)} chunks d·ªØ li·ªáu.")

    # 3. K·∫øt n·ªëi & Embed
    client = get_qdrant_client()
    embeddings_model = get_embeddings()
    collection_name = settings.QDRANT_COLLECTION_NAME

    # Ki·ªÉm tra k√≠ch th∆∞·ªõc vector
    sample_embedding = embeddings_model.embed_query("test")
    vector_size = len(sample_embedding)

    # T·∫°o collection n·∫øu ch∆∞a c√≥
    if not client.collection_exists(collection_name):
        print(f"üÜï T·∫°o m·ªõi Collection '{collection_name}' (size={vector_size})...")
        client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=vector_size, distance=Distance.COSINE),
        )

    # 4. Upload l√™n Qdrant (Batching ƒë·ªÉ ch·∫°y nhanh h∆°n)
    print("üöÄ B·∫Øt ƒë·∫ßu Embedding v√† Upload l√™n Qdrant...")

    batch_size = 50  # X·ª≠ l√Ω 50 ƒëo·∫°n m·ªôt l√∫c
    points = []

    for i, chunk in enumerate(all_chunks):
        # Embed n·ªôi dung
        vector = embeddings_model.embed_query(chunk.page_content)

        # T·∫°o payload (d·ªØ li·ªáu l∆∞u tr·ªØ)
        payload = {
            "content": chunk.page_content,
            "source": chunk.metadata.get("source"),
            "chunk_index": i
        }

        points.append(PointStruct(
            id=str(uuid.uuid4()),
            vector=vector,
            payload=payload
        ))

        # N·∫øu ƒë·ªß batch ho·∫∑c l√† ph·∫ßn t·ª≠ cu·ªëi c√πng th√¨ upload
        if len(points) >= batch_size or i == len(all_chunks) - 1:
            client.upsert(
                collection_name=collection_name,
                points=points,
                wait=True
            )
            print(f"   ƒê√£ upload {len(points)} chunks...")
            points = []  # Reset batch

    print("‚úÖ Ho√†n t·∫•t qu√° tr√¨nh nh·∫≠p d·ªØ li·ªáu!")


if __name__ == "__main__":
    import_documents()
